# Nexus


An free, open source automated platform for solving optimal control and model prediction problems using global analysis and hybrid approaches in a platform independent way.
We implement applied computational multiphysics problems using mixtures of methods from different interdisciplinary communities and interrelate the similarities but clarify terminological issues.

These problems are naturally at the nexus of the convergence of control theory, systems theory, mathmatical finance, modern statistical learning, generalized optimization, signal processing, machine learning, with information theoretic-inspired adaptive visualization and multilayer hybrid control.

We aim to implement an modular system to perform digital twinning and remotely control complex systems, and model register level hardware in multiple RTL-like platforms (at the computer architecture level).

We know that various new generations of digital devices, for example, augmented reality, as well as more world wide bandwidth of internet (aka 5g and satellite-based internet) are coming soonish, therefore, we gift the foresight of recognizing the differences of people who need to denote the differences between those who know medicine and snake charmers or false prophets.
We think people who spend their "continous feeling" time for example, ethanographers, who use <i class="fas fa-sign-language    ">signs</i> to record the habits and learn the <i class="fa fa-language" aria-hidden="true"><i class="fas fa-assistive-listening-systems    ">thought patterns</i></i><i class="fal fa-speakap    ">passively</i> to those who are aboriginal people.
We think because of the unlimited amount of information available these days, it is a good idea to always be learning and reinforcing shared valuation systems you see. Most ideas are not original, and in the digital world, you an expect almost everything to be in for example, to be digitized in the semantic web of knowledge (this called RDF).  I would recommend changing
the broken systems you see by offering feedback. In return certain "thought viruses", such as the GDPR and the CCPA will act as forcing functions to mutually enforce digital checks and universal balances towards a global mean field. We think very globally, but because of our experience in the mind's eye of a optimizing compiler, we see very self-similarities across the world,
and that may be exercabated by the internet. Remember, what you spend reading becomes the truth in your mind. This is called "reading". In digital world, you can expect that eventually people who have built for example, matrix factorization algorithms (i.e, the Netflix problem) will eventually see it from the human side of it. Why does it make sense to incentivize "influencers" on social media for example,
dead cats on <i class="fas fa-instagram    ">Instagram</i>? This is the "famous" poly-augmented advertising business model that seems to not have social value. But I remember authentically seeing the lack of signal processing in Ft. William (aka Calcutta, India, and thus, Sanskrit poetry was transfered over the Ganges River plain over to Tibet, according to the 14th Dali <i class="far fa-lightbulb-exclamation    "></i>)


We glue togther a stack of Rust, WebAssembly, and a safe implementation of server-side javascript (deno), and implement an alternative implementation of Julia for numerical computation.
We create a new language inspired by Simula67 and Smalltalk but modernize it significantly.  We focus leveraging on the fruits of information theory to create adaptive documentation, uncluttered display,
and cooptimize bandwidth, power, energy, transmission, and security purposes.

We focus on education, sharing of common methodology, translating between systems of common conventions, understanding the power of abstraction, specialization,
 and artisanal tool generation, with a laser focus on lowering of barriers to access "complicated" things, via automation of rote programming,
 securitization and enablement of functionalization and operational details.

We intend to create for example:
- a generic <i class="fa fa-google" aria-hidden="true">search engine</i> by indexing nothing, but using on the fly statistical independance tests across multiple search engines (this is the bootstrap method, this used to be for example, what metacrawler used to do, instead of the random surfer problem). We use this to do a more important "morphological analysis" of taxa, for example what is more important to improve the computational models of codons in the life, natural and social sciences.
- a generic <i class="fa <i class="fa fa-facebook" aria-hidden="true">social network</i> by using the opposite method of the graph reachability problem (aka the mark zuckerberg algorithm).
- we contribute to <i class="fas fa-wikipedia-w    ">the general source of truth, wikipedia</i> by adding sources to additional books that we come across. Since generation Z generally sees wikipedia as good way to see truth, and google as a good way to find stuff (which also semantically indexes google), we think this is a good feedback loop to create.
- we find ways to help with i18n (internationalization) and a11y (<i class="far fa-universal-access    "><i class="fas fa-accessible-icon    ">accessibility</i></i>) efforts of the world wide web. Usually helping out here: https://openlibrary.org/volunteer
- we encourage reproducable research, open access journals, and intend to, for example, use arXiv as a good representative model where we can "read" LaTeX equations and convert them to concrete interactive code rather than static PDFs, where we then send the resultant "assets" back to the original scholar for critical review.
- we consider other canonical perspectives for example, the OECD has started publishing original work on the standard library of genesis: https://custodians.online/ We share Alexandra Elbakyan and the vision of Aaron Swartz (RIP).
- we keep in mind that there are approximately three internets. The open world wide internet made possible by parsimony (in practice due to the Telecommunications Act of 1996 and The Digital Millennium Copyright Act of 1998), then the Russian internet, and the Chinese internet.
   https://www.copyright.gov/legislation/dmca.pdf and )
- we try not to make Sir Tim Bernes-Lee <i class="fas fa-sad-cry    ">sad</i> about the effects of the world wide web. CERN (those who run physical experiements at very high energy levels) has required tremendous international cooperation to figure the natural world. It is hard for various people to understand why they need to spend billions of dollars, or why for example,
the European Union needs to redundantly add a 3rd set of global positioning satellites in space just in case the two hedgemons in the world went to war. Unfortunatly, people frequently forget past history, for example, the horrors of World War II, or the historical revisionism of Stalinism, or the patholgoies of the arms trade. In spirit of (dead) information scientist David J.C. MacKay, we agree that the arms trade should be stopped (see: https://www.inference.org.uk/itprnn/book.pdf) but insist that mutually assured global investments in decreasing the fragility of the planet need to be urgently focused on. For example, simple things like hydrogen, the electron, lithium, salt and potassium "work" quite differently in different situations.
We need to computationally invest much more time doing uncertaintly analysis and numerial analysis and reduce barriers to access of things like climate and power/energy distribution.

Instead of the traditional two-valued boolean way of thinking common in computer science, we leverage the internet and common
sharing of best practices in the software development community to change perspectives and gauge your ability to adapt. The only constant in life is change,
but I beleive, after understanding for example, non-linear dynamics, our minds are actually very plastic, and can understand the patterns of change
very easily. We will share that gift by experimentation with the nexus of human computer interaction for the purposes of solving a manifest of
problems that I see (having worked in for example, the energy industry), but having an education of computer science and computer engineering, I know
control theory very well, thus in a digital setting, where I can perform all sorts of remote sensing, I reutilize recent advances in for example,
the behavioral sciences (for example, nudge theory), to "fix" things. For example, certain groups around the world, for example, in Britian, have
formed nudge groups where good practices structures are turned on by default (for example, automatic entry in some sort of 401k system makes sense
if you understand the actuarial sciences properly).


The ability to
rationally think with different perspectives and embody cognition is our best gift. Roger Martin in "The Opposable Mind" calls this
“opposable” thinking, where winning can become an end in itself (common in combative tone of trial law or partisan
politics). A better approach is to look at it from a game theoretic perspective and understand and question why certain traits no longer
make sense when there is, from an need to perform acturial perspective (those who need to for example, price insurance based on risk), there will have to be
an inevitable policy response (see: https://www.unpri.org/inevitable-policy-response/what-is-the-inevitable-policy-response/4787.article). This
requires building resilence and better advanced tooling for policy making across all 192 countries. We follow well known OECD and IMF guidelines
available on the open and clear internet, and frequently comment with lemons in atonal languages to rhetorically argue why it is not in shared interests
to overconsume natural resources (with libfossil) in the only tiny planet we have.




We aim to attract both newbies interested in computational science, experienced computer architects, academics and theoreticans alike from every
discipline who wants to do "computational <you insert it here>". We will preach that the most complex problems deal with societical issues, so we desire usage and consumption of as much diverse perspectives as possible. Spread that quine!



We aim to inspire everyone to become natural philosphers again, and learn by doing and experincing nature and the how the world works, at least for the time being. This has deep implications in terms of what energy is, where people should spend their energies, what is useful work, and how useful work is accounted for. Now days, one can use computers to do "cheap thought experiments", but framing the problem is most of the issue, and uncertainity and risk must be quanitifed rigorously. In the spirit of Douglas Hofstadter's novel "Gödel, Escher, Bach: an Eternal Golden Braid" and Donald Knuth's the Art of Science of Technology, and Richard Feynman's Lectures of Physics as they inspired generations of Computer Scientists, Mathmaticans, and Engineers. These days, their work is great, but I think we can do better due to better tooling. A better focus is adaptiveness, simplification, and computer-aided self-discovery. We write a system we wish our 5 year old selves had.  This vastly helps people internalize complicated what they perceive as complicated to both understand and fix complex systems.  We show how modern technology can vastly simplify people's via conceptual knowledge via modern web (primarily the efforts of memembers of the IETF and w3c) via assistive educational technology, as well as the efforts of the IEEE in previous years to standardize communications technology.


Read our philosophy for why we will implement in this way in [DESIGN.md]

What is a computer?

1.Movement of information - reading a magnetic tape and printing on a lin eprinter; reading
the input from a keyboard and storing in random access memory; reading the input from a
"joystick" and moving a picture on a screen.
2. detecting < class="fab fa-sign-in-alt fa-rotate-180"><i class="fas fa-google   "><i class="far fa-stumbleupon-circle   ">.<i class="fab fa-clipboard-prescription    ">places <i class="fab fa-spider-black-widow    "><i class="fas fa-xing-square    "><i class="fas fa-digital-tachograph    ">great firewalls <i class="fal fa-expeditedssl    "></i></i></i></i></i></i></i></i> intelligence from randomness, for example, chimps typing on keyboards (commonly old known on <i class="fal fa-wikipedia-w    ">ROC CURVE/i>.
These days, you could for want the <i class="fal fa-android    "><i class="far fa-servicestack    "><i class="fa fa-hashtag" aria-hidden="true">sash-sometimes-self-oraganizing<i class="fa fa-map" aria-hidden="true"><i class="fas fa-google    ">servo</i><i class="far fa-hire-a-helper    "><i class="fa fa-internet-explorer" aria-hidden="true">gopher</i>cnand</i>xor</i>lexeme</i>semantics</i>syntax</i><i class="fas fa-node    "><i class="fal fa-toolbox wavelet">curvelet</i>valve</i>games<i class="fa fa-barcode" aria-hidden="true">bellview<i class="fab fa-scanner    ">scanned in "china"</i><i class="fas fa-steam    "><i class="fas fa-route-interstate    ">I-5</i>Rust</i><i class="fa fa-behance" aria-hidden="true">sash</i></i>)
2.Arithmetic calculations - we have seen some examples of this already.
3.Comparison of information and choice of actions depending on the result
will look at the basic mechanisms for this.

Bridging is followed by a stage which Pickering describes as transcription, in which ideas and techniques from the existing domain are applied, in a more or less routine manner, to the new area.

Tarski - famous logican that idioated the axiomization of "set theory"
  - distinction between purely syntactic accounts of formal languages and a semantic treatment
    - meaning of a whole expression is arrived at depends solely on its syntactic construction
    - built upon a relationship of denotation, or designation, between the terms of a language
and the objects and properties in a suitable domain of interpretation.
  ` - in modern times, many people in the math profession consider variations of set theory as the "foundations" of math.

 Mac Lane - "accidently" created the formalization of category theory that took two mathamatical objects (natural transforations and adjunctions, and ideated a method of "morphisms"), which is very helpful to advance your maturity of the usefulness of mathmatics. This lets you reformalize and create new methods of
    - proofs with a limited number of axioms. This lets you generalize things way more than prior systems of axiomation.
    - in modern theoretical computer science, category theory is probably more influential in the haskell and caml (). For example, the Caml series of languages is literally called the "Categorical Machine".
    - I took CS101 using a language called Squeak. It had a morphic interface and was written in SmallTalk. It was a categorictheoretical lang. SmallTalk can always be resurrected because it can self host.
    - Lession learned: sometimes the best "tech" doesn't win. Other market factors can be in play. For example, Bell Labs, part of AT&T gave away C and UNIX. Dangling pointers from C still cause plagues of security problems. modern C++ is great, but it has a lot of old crud.


Morris - theory of signs - differentiated semiosis as process between signing, designatum, and interpretant - semantics - the relations of signs to the objects to which the signs are applicable - pragmatics - the relation of signs to interpreters

Carnap - intensional/extensiona - “pragmatics”, “semantics” and “logical syntax”, should be studied as part of a theory of language

Shannon - “the problem of constructing a computing routine or ‘program’ for a modern general-purpose computer which will enable it to play

Shannon/Cherry/Turing - Just as arithmetic has led to the design of computing machines, so we may perhaps infer that symbolic logic may lead to the evolution of “reasoning-machines” and the mechanization. of thought processes."

Aiken/Hopper - automate small number of ops on large data sets - The development of numerical analysis . . . [has] reduced, in effect, the processes of mathematical analysis to selected sequences of the five fundamental operations of arithmetic: addition, subtraction, multiplication, division, and reference to tables of previously computed results. The automatic sequence controlled calculator was designed to carry out any selected sequence of these operations under completely automatic control

Eckert -  The device should be as simple as possible, that is, contain as few elements as possible. This can be achieved by never performing two operations simultaneously, if this would cause a significant increase in the number of elements required. The result will be that the device will work more reliably . . .
  - It is also worth emphasizing that up to now all thinking about high speed digital computing devices has tended in the opposite direction: Towards acceleration by telescoping processes at the price of multiplying the number of element

Eckert and Mauchly "described an order code, known as ‘Code A’, which included a pair of ‘comparison’ orders. In Code A, each order could contain the addresses of up to three storage locations, alpha, beta and gamma. The effect of the order c was described as follows:
If the number stored in register alpha is greater than the number stored in register beta, shift"




Gill - first notation

Laning and Zierler’s program w - The basic idea underlying formula translation systems was the recognition that mathematical formulae encode algorithms which can be translated into machine code.

Fortran - limited to artihmetic formula =- This definition emphasises both the similarity between Fortran and mathematics, and also the possibility of difference: the last sentence, for example, rules out the use in Fortran of juxtaposition to express multiplication. It relies entirely on readers’ intuition, however, about what constituted ‘normal’ mathematical expressions.

The UNCOL project drew a distinction between machine or computer-oriented languages (COLs) and problem-oriented languages (POLs). Rather than trying to define a single problem-oriented language, like Algol, the idea behind the project was to define a universal computer-oriented language, or UNCOL. This language would be at a higher level than machine code, but at a lower level than languages like Fortran or Algol. An implementation of UNCOL was to be written for every target machine, but because of its lower level, it was felt that this would be easier than implementing a language like Algol across a range of hardware.

LISP - 1965: “The prize to be won if we can develop a reasonable mathematical theory of computation is the elimination of debugging”.

Information algebra - A model constructed using the information algebra, therefore, contained no direct
representation of the entities being modelled. An entity was represented solely as the collection of the values of its properties at a given time. A consequence of this type of representation is that, over time, a given entity would be represented by many different points in property space, as the values associated with its various properties changed. The model itself provided no representation of the fact that these are properties of the same entity at different times“the concepts of Modern Algebra and Point Set Theory”, which would guide the development of future programming languages. The report enumerated various shortcomings in existing languages, and hoped to address them largely by defining a declarative rather than a procedural framework.
The designer of an information system for a particular application should begin by defining all the relevant properties of the entities involved in the application. With each property was associated a value set.


Gorn - Programming Languages and Pragmatics”:50 some sessions were devoted to topics in the machine processing of languages, such as ‘translation’ and ‘interpretive assembly’, while others considered the requirements for programming languages that were to be used in specific application areas, such as real-time applications and information retrieval. In an overview paper, Heinz Zemanek listed four specific areas as being relevant to the pragmatics of programming languages—compilers,
hardware and operating systems, intended application areas and human users—but

Ross - oss had proposed a general technique for modelling data about an arbitrary collection of entities using ‘plexes’ of n-component elements. Rather than being based on an abstract data space, how- ever, Ross’s proposal was based on an abstract view of a computer’s memory, in which representations of distinct entities which happened to share the same properties could easily coexist at different locations in the store. They would be distin-

Dikstra - Ideated the idea of structured programming which was in effect a "design pattern" - a recommended set of control and data structures, but also a concern with the idea of the provable correctness of programs together with the constructive method by which such programs could be produced, and finally a general scheme for program modules.
Dijkstra drew upon the old idea of program semantics being
given in terms of a virtual machine: “Between two successive pearls we can make a
‘cut’, which is a manual for a machine provided by the part of the necklace below
the cut and used by the program represented by the part of the necklace above the". I read most of dijkstra's notes (I lived in Austin and )



Algol research programe introduced a general model of program development where programs were systematically derived from specifications by a process of refinement.
This process could be carried out formally, but more often it was not; however, even informal versions of the process, which included testing, were found useful and widely adopted by industry.
Rather than being completely opposed techniques with nothing in common, as McCarthy and Dijkstra suggested, proof and testing came to be viewed as complementary techniques for ensuring the correctness of software
within the context of refinement-based methods.  The origins of Algol lay in the tradition of scientific programming carried out in the 1950s. A typical problem in this tradition was to devise an algorithm to carry out a particular computation, and Algol was conceived as a language for expressing and communicating such algorithms.


said to be those which make some testable, empirical assertion. extant, now often referred to as ‘agile’ or ‘iterative and incremental’ development.

Friedrich Bauer wrote in an
overview of the young field of software engineering that the aim of the discipline
was “to obtain economically software that is reliable and works efficiently on real life problems"

Parnas 1972 — Decomposing Systems into Modules

CLU — 1974–5 — rep, up, down and cvt

Bachman - As Charles Bachman, a database researcher, put
it in 1973, the move from files to database could be viewed as a kind of Copernican
revolution, challenging the perceived centrality of programs and proposing a new
model of computation in which programs were viewed as satellites of a central data. Secondly, as databases are assumed to be independent of particular programs,
programs using them cannot in general use the address of a data item in memory as a way to access it. Entities can only be identified in a database by looking at the actual data values stored for each.
For this to be possible, records must have some unique attribute distinguishing them from all other records in the file. Entities often do not have this property: for example, we cannot assume that the individuals in a group of people will be uniquely identified by their names. To get round this problem, the records in a database typically include an attribute or attributes, known as a key, whose value is guaranteed to be unique within the file. Bachman described the way a programmer worked with network databases as
“navigation”, and a similar metaphor was used by others, such as Jay Earley who referred to “access paths” through data structures.
‘current locations’ within a database, and by making use of commands embedded in a programming language could update the current location and thereby move from one data item to another.


Codd - 1970 -  Created the relational data model which was based on a single structuring concept, the relation. This is basically the set-theoretical concept of a relation, or Cartesian product of sets, used here as a formal model of records. No special data structure, such as the sets in a network database, was used to model relationships between entities. Rather, relationships were modelled by pairing up the key fields of the related entities, and storing these pairs in a further relation. So whereas network databases had two primitive concepts, files and sets, corresponding to the informal notions of entity and relationship, relation databases had one primitive concept, the relation, which modelled both. One advantage claimed for this was that it kept the logical structure of the data independent of its physical representation, thus making updates and modifications to the storage strategy easier, because they would not necessarily imply changes to the application programs using the database. Secondly, data manipulation in the relational model did not proceed by means of record-at-a-time navigation through the database. Rather, a number of high-level operations on relations were provided

Simula67- the first "object oriented language"
Simulation languages, then, were not simply alternatives to algorithmic languages like Algol, but had an entirely different goal: their purpose was to describe not just computations, but whole system.
Rather than creating a new notation, Dahl and Nygaard chose to base Simula on Algol 60; they did this in such a way that it was an extension of Algol, in the sense that any feature or structure defined by Algol 60 could be used in a Simula program.
The need to describe the active behaviour of processes meant that a simulation language had to contain at least some elements of an algorithmic language.



Hoare/Wirth- Pascal and the object/attribute/class/reference record model:
between the real world and the computer model in terms of four properties. Firstly, objects were considered to possess a number of attributes represented by fields in a record, each of which could hold a data value describing the object. Secondly, similar objects would naturally have the same kinds of attributes, though normally holding different values. Objects could therefore be grouped into classes, and in a program the attributes belonging to a particular class of objects would be defined by a record class.
Next, it was also considered important to model relationships between objects: in the simple case of functional relationships, this was done by defining a new kind of data value, which defined a reference to a record, and allowing records to hold references to other records to which they were related.
Records - It was recognized that many classes consisted of disjoint subclasses of objects, in the way that the class of vertebrates consists of the subclasses of mammals, birds and so on. The proposals allowed record classes to contain subclasses, with ‘private’ fields that defined attributes that applied only to objects belonging to certain subclasses



Simula78- Two significant aspects distinguished Simula 67’s prefix classes from the record subclass proposed by Hoare. Firstly, prefix classes are more flexible than record subclasses.
Hoare’s proposal required all the subclasses of a given record class to be specified at the point of definition of the class. In Simula, on the other hand, any class can be used as prefix in any other class definition, giving an ability to reuse code that went beyond that offered by record subclasses
  - A significant innovation in Simula 67 was the introduction of prefix classes, The idea was that the definition of a new class could specify a single prefix class: the attributes of the prefix class would become attributes of the new class,
  and further attributes could be added to specialize the concept being modelled by the class. Class prefixing could be carried out repeatedly as often as required, allowing a hierarchy of classes to be define record class definition for every type of data that was to be stored in a list
  - A second point of difference was the notion of virtual quantities. Using this mechanism, a prefix class could declare a field which it does not itself define, but which it is planned will be defined in subclasses. For example, a vehicle class might define a field called ‘capacity’,.


Smalltalk vs Simula67: http://simula67.at.ifi.uio.no/Archive/AP-Black-talk-2011.pdf
 beautifully Smalltalk works as a mirror in which we can see our own understanding.
 Smalltalk-72 was clearly inspired by Simula
  It took:
    Classes, Objects, Inheritance, Object References
    It refined and explored:
    Objects as little computers: “a recursion on the
    notion of computer itself” [Kay1993]
  Objects combining data and the operations on
  that data
  It dropped:
  bjects as processes, classes as packages

US & Scandinavian Objects:
Feature       Simula 67    Smalltalk 80  Snyder (1991)
Abstraction: "Modelling": attributes      Objects characterized
              attributes  encapsulated    by offered services
              exposed
Active Objects Yes        No             "Associated Concept"
Dynamic Objects Yes Yes Yes
Classes Yes Yes "Shared implementations"
Subclassing Yes Yes "shared partial mimplementations"
Overriding under control    under control of   optional; delegation permitted
           of superclass    subclass
Classes as
packages
Yes No N      o

Simula doesn't mention abstraction specifically. It speaks of “modelling"
a model: an abstraction with a mission
The idea of separating the internal (concrete) and external (abstract) views of data was yet to mature



useful apis: https://docs.microsoft.com/en-us/graph/onenote-get-content
